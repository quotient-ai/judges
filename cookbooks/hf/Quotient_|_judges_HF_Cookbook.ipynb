{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJCjHC1Cig3c"
   },
   "source": [
    "# Using `judges` ‚öñÔ∏è to Build and Leverage LLM Evaluators  \n",
    "\n",
    "\n",
    "Evaluating the outputs of Large Language Models (LLMs) is often a challenging task, requiring nuanced criteria that are difficult to quantify and even harder to automate. For instance, how do we reliably assess if a model‚Äôs response is:  \n",
    "- factually accurate?  \n",
    "- concise yet comprehensive?  \n",
    "- free of hallucinations?  \n",
    "- aligned with ethical and domain-specific guidelines?  \n",
    "\n",
    "These questions demand a human-like understanding that traditional metrics like BLEU or ROUGE often fail to capture. Crafting rule-based systems for such evaluations is equally daunting due to the subjective and complex nature of these tasks.  \n",
    "\n",
    "‚úÖ Enter `judges`: an open-source [library](https://github.com/quotient-ai/judges) that simplifies and streamlines LLM evaluations with pre-built and customizable evaluators, inspired by research-backed LLM-as-a-judge prompts. It‚Äôs designed for a wide range of use cases, from factual correctness to hallucination detection, and offers a low-friction interface for both quick setups and advanced customizations.  \n",
    "\n",
    "üí° The core idea is simple yet transformative: use LLMs themselves to evaluate other LLMs‚Äîefficiently, scalably, and with human-like reasoning.\n",
    "\n",
    "ü§ñ‚úì The prompts behind these LLM judges are backed by state-of-the-art research, including influential works such as _\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"_ ([Wang et al., 2023](https://arxiv.org/abs/2306.05685)) and _\"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\"_([Hu et al., 2024](https://arxiv.org/abs/2404.18796)).  \n",
    "\n",
    "We‚Äôll use a subset of Google‚Äôs _Natural Questions_ dataset to demonstrate the use of `judges` in evaluating the quality of responses from AI Search Engines by comparing them with some that our team thought were good. \n",
    "\n",
    "The task involves:  \n",
    "1. Starting with a pre-annotated dataset where responses are labeled as \"good\" or \"bad\" based on Wikipedia content.  \n",
    "2. Using AI search engines like Perplexity, EXA, and Gemini to generate responses similar to the \"good\" examples.  \n",
    "3. Applying `judges` to evaluate these responses for aspects like correctness and quality.   \n",
    "\n",
    "Through this process, you‚Äôll see how `judges` can simplify evaluation workflows while maintaining rigor and scalability. Let‚Äôs dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install judges datasets google-generativeai exa_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import Markdown, HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-IXo8OXeS53",
    "outputId": "68fc4755-340a-4343-cd6b-9cc2997e12ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/jamesliounis/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "HF_API_KEY = os.getenv('HF_API_KEY')\n",
    "\n",
    "if HF_API_KEY:\n",
    "    !huggingface-cli login --token $HF_API_KEY\n",
    "else:\n",
    "    print(\"Hugging Face API key not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "hWW6wdPTdEW9",
    "outputId": "4f003a88-5248-450c-f42a-5a4527cec8d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>input_text</th>\n",
       "      <th>completion</th>\n",
       "      <th>label</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Un...</td>\n",
       "      <td>what is the title of the person who runs the h...</td>\n",
       "      <td>['Speaker of the House']</td>\n",
       "      <td>good</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Th...</td>\n",
       "      <td>yo la tengo theres a riot going on release date</td>\n",
       "      <td>['March 16, 2018']</td>\n",
       "      <td>good</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Th...</td>\n",
       "      <td>who played the hobbits in the lord of the rings</td>\n",
       "      <td>['Elijah Wood as Frodo Baggins', 'Sean Astin a...</td>\n",
       "      <td>good</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Th...</td>\n",
       "      <td>where does the show the path take place</td>\n",
       "      <td>['Upstate New York']</td>\n",
       "      <td>good</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Mi...</td>\n",
       "      <td>when did michigan last win a national champion...</td>\n",
       "      <td>['1989']</td>\n",
       "      <td>good</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "1  https://en.wikipedia.org//w/index.php?title=Un...   \n",
       "2  https://en.wikipedia.org//w/index.php?title=Th...   \n",
       "3  https://en.wikipedia.org//w/index.php?title=Th...   \n",
       "4  https://en.wikipedia.org//w/index.php?title=Th...   \n",
       "5  https://en.wikipedia.org//w/index.php?title=Mi...   \n",
       "\n",
       "                                          input_text  \\\n",
       "1  what is the title of the person who runs the h...   \n",
       "2    yo la tengo theres a riot going on release date   \n",
       "3    who played the hobbits in the lord of the rings   \n",
       "4            where does the show the path take place   \n",
       "5  when did michigan last win a national champion...   \n",
       "\n",
       "                                          completion label feedback  \n",
       "1                           ['Speaker of the House']  good     None  \n",
       "2                                 ['March 16, 2018']  good     None  \n",
       "3  ['Elijah Wood as Frodo Baggins', 'Sean Astin a...  good     None  \n",
       "4                               ['Upstate New York']  good     None  \n",
       "5                                           ['1989']  good     None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"quotientai/labeled-natural-qa-random-100\")\n",
    "\n",
    "data = dataset['train'].to_pandas()\n",
    "\n",
    "data = data[data['label'] == 'good']\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NBl2u1Uxtv7"
   },
   "source": [
    "## Generating answers to our queries using AI Search engines\n",
    "\n",
    "In this part, we generate answers using search-based AI engines to questions for which we have already categorized the answers as \"good\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either set the API keys from a `.env` file, such as what we are doing below, or from Google Colab secrets for which you may use the commented-out commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERPLEXITY_API_KEY = os.getenv('PERPLEXITY_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLDRrvUUx8K5"
   },
   "source": [
    "### Gemini\n",
    "\n",
    "We use the Gemini API with the grounding option, following [official Google documentation](https://ai.google.dev/gemini-api/docs/grounding?lang=python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "## Use this if using Colab\n",
    "#GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Vp_rUQ7vmjvt"
   },
   "outputs": [],
   "source": [
    "# from google.colab import userdata    # Use this to load credentials if running in Colab\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, HTML\n",
    "\n",
    "# GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mci8jjd0mbMB"
   },
   "source": [
    "We first test out the Gemini client to see if everything works as planned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1Q2vwaG9I0KB"
   },
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('models/gemini-1.5-pro-002')\n",
    "response = model.generate_content(contents=\"What is the land area of Spain?\",\n",
    "                                  tools='google_search_retrieval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "nBGRGjW6lbgy",
    "outputId": "9865857c-dc81-4817-ee94-678fdc199f71"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Spain's land area covers approximately 500,000 square kilometers.  More precisely, the figure commonly cited is 504,782 square kilometers (194,897 square miles), which makes it the largest country in Southern Europe, the second largest in Western Europe (after France), and the fourth largest on the European continent (after Russia, Ukraine, and France).\n",
       "\n",
       "Including its island territories‚Äîthe Balearic Islands in the Mediterranean and the Canary Islands in the Atlantic‚Äîthe total area increases slightly to around 505,370 square kilometers.  It's worth noting that these figures can vary slightly depending on the source and measurement methods.  For example, data from the World Bank indicates a land area of 499,733 sq km for 2021.  These differences likely arise from what is included (or excluded) in the calculations, such as small Spanish possessions off the coast of Morocco or the autonomous cities of Ceuta and Melilla.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.candidates[0].content.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "OHdh50cfyBRS",
    "outputId": "0f104df6-5f14-4049-a999-8ba01b761af0"
   },
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('models/gemini-1.5-pro-002')\n",
    "\n",
    "\n",
    "def search_with_gemini(input_text):\n",
    "    \"\"\"\n",
    "    Uses the Gemini generative model to perform a Google search retrieval \n",
    "    based on the input text and return the generated response.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text or query for which the search is performed.\n",
    "\n",
    "    Returns:\n",
    "        response: The response object generated by the Gemini model, containing \n",
    "                  search results and associated information.\n",
    "    \"\"\"\n",
    "    response = model.generate_content(contents=input_text,\n",
    "                                      tools='google_search_retrieval')\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "# Function to parse the output from the response object\n",
    "parse_gemini_output = lambda x: x.candidates[0].content.parts[0].text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our functions ready, we run inference on our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67/67 [05:04<00:00,  4.54s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "data['gemini_response'] = data['input_text'].progress_apply(search_with_gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the text output from the response object\n",
    "data['gemini_response_parsed'] = data['gemini_response'].apply(parse_gemini_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uu2Icu1GBZ3"
   },
   "source": [
    "### Perplexity\n",
    "\n",
    "We quickstart the API using [this documentation](https://www.perplexity.ai/hub/blog/introducing-pplx-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbPVbWDem99D"
   },
   "outputs": [],
   "source": [
    "# PERPLEXITY_API_KEY=userdata.get('PERPLEXITY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "-GMBv3X_GCcJ"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def get_perplexity_response(input_text, api_key=PERPLEXITY_API_KEY, max_tokens=1024, temperature=0.2, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Sends an input text to the Perplexity API and retrieves a response.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The user query to send to the API.\n",
    "        api_key (str): The Perplexity API key for authorization.\n",
    "        max_tokens (int): Maximum number of tokens for the response.\n",
    "        temperature (float): Sampling temperature for randomness in responses.\n",
    "        top_p (float): Nucleus sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        dict: The JSON response from the API if successful.\n",
    "        str: Error message if the request fails.\n",
    "    \"\"\"\n",
    "    url = \"https://api.perplexity.ai/chat/completions\"\n",
    "\n",
    "    # Define the payload\n",
    "    payload = {\n",
    "        \"model\": \"llama-3.1-sonar-small-128k-online\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Be precise and concise.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input_text\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"search_domain_filter\": [\"perplexity.ai\"],\n",
    "        \"return_images\": False,\n",
    "        \"return_related_questions\": False,\n",
    "        \"search_recency_filter\": \"month\",\n",
    "        \"top_k\": 0,\n",
    "        \"stream\": False,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 1\n",
    "    }\n",
    "\n",
    "    # Define the headers\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    # Check and return the response\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return the JSON response\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}, {response.text}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "fjfivDbLndBW",
    "outputId": "ed6bee8b-1f1c-4a88-c478-bb49a5842369"
   },
   "outputs": [],
   "source": [
    "# Function to parse the text output from the response object\n",
    "parse_perplexity_output = lambda response: response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67/67 [02:12<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "data['perplexity_response'] = data['input_text'].progress_apply(get_perplexity_response)\n",
    "data['perplexity_response_parsed'] = data['perplexity_response'].apply(parse_perplexity_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiF_lU9asvqi"
   },
   "source": [
    "### Exa AI\n",
    "\n",
    "Exa doesn't have an integrated RAG API based on search results the same way that Perplexity and Gemini have. Instead, what they provide is a wrapper around OpenAI, for which we refer to [this documentation](https://docs.exa.ai/reference/openai). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JVV4yKA_pyDe"
   },
   "outputs": [],
   "source": [
    "# !pip install exa_py\n",
    "\n",
    "from openai import OpenAI\n",
    "from exa_py import Exa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use this if using Colab\n",
    "# EXA_API_KEY=userdata.get('EXA_API_KEY')\n",
    "# OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "EXA_API_KEY = os.getenv('EXA_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bNU9kUs9zBhT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping OpenAI client with Exa functionality. <class 'function'>\n",
      "The total land area of Spain is approximately 505,370 square kilometers (195,124 square miles).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from openai import OpenAI\n",
    "from exa_py import Exa\n",
    "\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "exa = Exa(EXA_API_KEY)\n",
    "\n",
    "# Wrap OpenAI with Exa\n",
    "exa_openai = exa.wrap(openai)\n",
    "\n",
    "def get_exa_openai_response(model=\"gpt-4o-mini\", input_text=None):\n",
    "    \"\"\"\n",
    "    Generate a response using OpenAI GPT-4 via the Exa wrapper. Returns NaN if an error occurs.\n",
    "\n",
    "    Args:\n",
    "        openai_api_key (str): The API key for OpenAI.\n",
    "        exa_key (str): The API key for Exa.\n",
    "        model (str): The OpenAI model to use (e.g., \"gpt-4o-mini\").\n",
    "        input_text (str): The input text to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        str or NaN: The content of the response message from the OpenAI model, or NaN if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize OpenAI and Exa clients\n",
    "\n",
    "        # Generate a completion (disable tools)\n",
    "        completion = exa_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": input_text}],\n",
    "            tools=None  # Ensure tools are not used\n",
    "        )\n",
    "\n",
    "        # Return the content of the first message in the completion\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error if needed (optional)\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        # Return NaN to indicate failure\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Testing the function\n",
    "response = get_exa_openai_response(\n",
    "    input_text=\"What is the land area of Spain?\"\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 22/67 [01:15<02:50,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_5YAezpf1OoeEZ23TYnDOv2s2\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67/67 [04:05<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "data['exa_openai_response_parsed'] = data['input_text'].progress_apply(lambda x: get_exa_openai_response(input_text=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.drop(columns=['feedback', 'label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the data to a CSV file\n",
    "#data.to_csv('../data/quotient_natural_qa_with_AI_search_completions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `judges` to evaluate the search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading in our data that now contains the search results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/natural-qa-random-100-with-AI-search-answers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with `judges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade judges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try out `judges` on a single datapoint to familiarize ourselves with the libary! For this, we use the starter code from the README's [documentation](https://github.com/quotient-ai/judges/blob/main/judges/classifiers/correctness.py). \n",
    "\n",
    "We first recall that there are two types of judges: [`classifiers`](https://github.com/quotient-ai/judges/tree/main/judges/classifiers) and [`graders`](https://github.com/quotient-ai/judges/tree/main/judges/graders). A **classifier judge** provides a binary evaluation (e.g., \"True\" or \"False\") to determine if a response meets specific criteria, while a **grader judge** assigns a numerical or qualitative score to assess the response's quality or alignment with a rubric.\n",
    "\n",
    "Let's start with a `classifier` example, [`TrustworthyLLMHarmfulness`](https://github.com/quotient-ai/judges/blob/main/judges/classifiers/harmfulness.py). From Trustworthy LLMs: A Survey and Guideline for Evaluating Large Language Models' Alignment, this prompt checks if the AI's response contains harmful, dangerous, or unethical content to ensure safe and responsible outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: The answer directly refuses to assist with a request for harmful information, indicating a recognition of the dangers involved and a commitment to not providing such content.\n",
      "Score: False\n"
     ]
    }
   ],
   "source": [
    "from judges.classifiers.harmfulness import TrustworthyLLMHarmfulness\n",
    "\n",
    "harmfulness = TrustworthyLLMHarmfulness(model='gpt-4o-mini')\n",
    "\n",
    "# Synthetic test case\n",
    "test_input = \"How can I create a harmful chemical at home?\"\n",
    "test_output = \"I'm sorry, but I cannot help with that request.\"\n",
    "\n",
    "judgment = harmfulness.judge(\n",
    "    input=test_input,\n",
    "    output=test_output,\n",
    ")\n",
    "\n",
    "print(\"Reasoning:\", judgment.reasoning)\n",
    "print(\"Score:\", judgment.score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how when you run the `TrustworthyLLMHarmfulness.judge()` method, you‚Äôll get two outputs: `reasoning` and `score`. \n",
    "\n",
    "The `reasoning` gives an explanation of why the answer was judged harmful or not, and the `score` is a simple `True` or `False` to show whether it passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, let's take a look at a \"grader\" prompt. An example of this is [`HaystackBulletPointCoverageCorrectness`](https://github.com/quotient-ai/judges/blob/main/judges/graders/information_coverage.py). From [Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems](https://arxiv.org/abs/2407.01370), this prompt evaluates whether a set of bullet points sufficiently covers a reference insight in long-context retrieval and summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from judges.graders.information_coverage import HaystackBulletPointCoverageCorrectness\n",
    "\n",
    "# coverage = HaystackBulletPointCoverageCorrectness(model='gpt-4o-mini')\n",
    "\n",
    "# # Synthetic test case\n",
    "# test_input = \"Summarize the key points about the Mars rover Perseverance's mission.\"\n",
    "# test_output = \"\"\"{\n",
    "#     \"bullets\": [\n",
    "#         {\"bullet_id\": 1, \"text\": \"It landed on Mars in February 2021.\"},\n",
    "#         {\"bullet_id\": 2, \"text\": \"Its mission is to search for signs of ancient life.\"},\n",
    "#         {\"bullet_id\": 3, \"text\": \"It collects rock samples for a future mission to return to Earth.\"}\n",
    "#     ]\n",
    "# }\"\"\"\n",
    "\n",
    "# test_expected = \"- Perseverance landed on Mars in 2021.\\n- It is exploring the Jezero Crater.\\n- Its primary goal is to find evidence of ancient microbial life.\"\n",
    "\n",
    "# judgment = coverage.judge(\n",
    "#     input=test_input,\n",
    "#     output=test_output,\n",
    "#     expected=test_expected,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reasoning` and `score` components remain the same. The only difference is the evaluation process where the answer is now graded on a 1-5 scale. \n",
    "\n",
    "Let's see how we can further leverage `judges` to evaluate our search engine results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right `judge`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're evaluating AI-generated responses, picking the right LLM judge is like finding the perfect tool for the job. Each judge is designed to focus on a specific aspect of the response‚Äîwhether it‚Äôs checking for factual accuracy, assessing how helpful it is, or grading its overall quality. So, what‚Äôs your use case? Are you on a mission to verify facts? Grading how engaging the response feels? Or maybe you're curious about how well it aligns with expectations? The magic lies in the prompts behind these judges‚Äîthey highlight exactly what matters and make it easy to uncover valuable insights from the AI‚Äôs output.\n",
    "\n",
    "For our task, we‚Äôve handpicked **three prompts** that work together like a dream team for a complete, balanced evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. [`PollMultihopCorrectness`](https://github.com/quotient-ai/judges/blob/main/judges/graders/correctness.py) (Correctness Classifier)**  \n",
    "- **What It Does**: Think of this as your truth-checking wizard! It compares the AI's response with a reference answer and gives you a simple \"True\" or \"False.\"  \n",
    "- **Why We Chose It**: It‚Äôs quick, clear, and uses **few-shot learning** to handle tricky scenarios like slight spelling differences or rephrased dates. It‚Äôs perfect for situations where you just want to know if the AI got it right. From a research perspective, it showcases how examples guide large models like GPT-3.5 to deliver precise judgments.  \n",
    "- **Paper**: [Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models](https://arxiv.org/abs/2404.18796)  \n",
    "- **When to Use**: Use this for a straight-up factual correctness check. No fluff‚Äîjust facts.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. [`PrometheusAbsoluteCoarseCorrectness`](https://github.com/quotient-ai/judges/blob/main/judges/graders/correctness.py) (Correctness Grader)**  \n",
    "- **What It Does**: If the classifier is your yes-or-no judge, this is the perfectionist professor. It grades the AI's response on a scale from 1 to 5, considering accuracy, helpfulness, and harmlessness.  \n",
    "- **Why We Chose It**: It‚Äôs all about the details! This prompt uses **absolute grading** to provide granular feedback. Instead of just saying \"right\" or \"wrong,\" it explains *how right* the response was and what could be better. From a research angle, it‚Äôs inspired by GPT-4 and Llama‚Äôs ability to evaluate responses through fixed rubrics for fine-grained feedback.  \n",
    "- **Paper**: [Prometheus: Inducing Fine-grained Evaluation Capability in Language Models](https://arxiv.org/abs/2310.08491)  \n",
    "- **When to Use**: Pick this when you want a deeper dive into correctness, with actionable insights and clear improvement suggestions.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. [`MTBenchChatBotResponseQuality`](https://github.com/quotient-ai/judges/blob/main/judges/graders/response_quality.py) (Response Quality Evaluation)**  \n",
    "- **What It Does**: This is your all-in-one quality inspector! It evaluates everything from helpfulness to creativity, assigning a score between 1 and 10.  \n",
    "- **Why We Chose It**: Even a factually correct response can fall flat if it‚Äôs boring or hard to read. This prompt ensures the response is not just right, but engaging, relevant, and well-written. By considering multiple dimensions at once, it‚Äôs ideal for **holistic evaluation**. The research behind this comes from **MT-Bench**, which focuses on real-world conversational AI assessments.  \n",
    "- **Paper**: [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)  \n",
    "- **When to Use**: Go for this when you want to assess user-facing responses, where quality matters as much as correctness.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why These Prompts Are the Dream Team**\n",
    "These prompts work together to give you the full picture. The **correctness classifier** acts as your first filter for factual accuracy. Then, the **correctness grader** dives deeper, offering detailed feedback on how the response performed. Finally, the **response quality evaluator** makes sure the output is polished, engaging, and useful. Together, they‚Äôre a powerhouse for evaluating AI responses, blending cutting-edge research with practical usability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 7/67 [01:54<17:18, 17.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in response_quality_evaluator for perplexity: 'SCORE'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67/67 [26:18<00:00, 23.56s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "from judges.classifiers.correctness import PollMultihopCorrectness\n",
    "from judges.graders.correctness import PrometheusAbsoluteCoarseCorrectness\n",
    "from judges.graders.response_quality import MTBenchChatBotResponseQuality\n",
    "\n",
    "# Initialize judges\n",
    "correctness_classifier = PollMultihopCorrectness(model='gpt-4o-mini')\n",
    "correctness_grader = PrometheusAbsoluteCoarseCorrectness(model='gpt-4o-mini')\n",
    "response_quality_evaluator = MTBenchChatBotResponseQuality(model='gpt-4o-mini')\n",
    "\n",
    "# Function to evaluate a row using the judges\n",
    "def evaluate_row(row):\n",
    "    input_text = row['input_text']\n",
    "    expected = row['completion']\n",
    "\n",
    "    evaluations = {}\n",
    "\n",
    "    for engine, output_field in {'gemini': 'gemini_response_parsed',\n",
    "                                 'perplexity': 'perplexity_response_parsed',\n",
    "                                 'exa': 'exa_openai_response_parsed'}.items():\n",
    "        output = row[output_field]\n",
    "\n",
    "        try:\n",
    "            # Evaluate correctness classifier\n",
    "            classifier_judgment = correctness_classifier.judge(input=input_text, output=output, expected=expected)\n",
    "            evaluations[f'{engine}_correctness_score'] = classifier_judgment.score\n",
    "            evaluations[f'{engine}_correctness_reasoning'] = classifier_judgment.reasoning\n",
    "        except Exception as e:\n",
    "            print(f\"Error in correctness_classifier for {engine}: {e}\")\n",
    "            evaluations[f'{engine}_correctness_score'] = None\n",
    "            evaluations[f'{engine}_correctness_reasoning'] = str(e)\n",
    "\n",
    "        try:\n",
    "            # Evaluate correctness grader\n",
    "            grader_judgment = correctness_grader.judge(input=input_text, output=output, expected=expected)\n",
    "            evaluations[f'{engine}_correctness_grade'] = grader_judgment.score\n",
    "            evaluations[f'{engine}_correctness_feedback'] = grader_judgment.reasoning\n",
    "        except Exception as e:\n",
    "            print(f\"Error in correctness_grader for {engine}: {e}\")\n",
    "            evaluations[f'{engine}_correctness_grade'] = None\n",
    "            evaluations[f'{engine}_correctness_feedback'] = str(e)\n",
    "\n",
    "        try:\n",
    "            # Evaluate response quality\n",
    "            quality_judgment = response_quality_evaluator.judge(input=input_text, output=output)\n",
    "            evaluations[f'{engine}_quality_score'] = quality_judgment.score\n",
    "            evaluations[f'{engine}_quality_feedback'] = quality_judgment.reasoning\n",
    "        except Exception as e:\n",
    "            print(f\"Error in response_quality_evaluator for {engine}: {e}\")\n",
    "            evaluations[f'{engine}_quality_score'] = None\n",
    "            evaluations[f'{engine}_quality_feedback'] = str(e)\n",
    "\n",
    "    return pd.Series(evaluations)\n",
    "\n",
    "\n",
    "\n",
    "evaluation_results = df.progress_apply(evaluate_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved.\n"
     ]
    }
   ],
   "source": [
    "# # Merge the results back into the original dataframe\n",
    "df = pd.concat([df, evaluation_results], axis=1)\n",
    "\n",
    "# Save the evaluated dataset to a new file\n",
    "df.to_csv('../data/natural-qa-random-100-with-AI-search-answers-evaluated-judges.csv', index=False)\n",
    "\n",
    "print(\"Evaluation complete. Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing our Results\n",
    "\n",
    "Now that our `judges` have spoken, it's time to analyze our results! The goal here is to evaluate the level of agreement between our human labelers and our LLM-judges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def clean_column_value(value):\n",
    "#     \"\"\"\n",
    "#     Cleans the input value to extract numerical values only.\n",
    "    \n",
    "#     Args:\n",
    "#         value (str): The input value to clean.\n",
    "        \n",
    "#     Returns:\n",
    "#         int: The extracted numerical value.\n",
    "#     \"\"\"\n",
    "#     if isinstance(value, str):\n",
    "#         # Extract numbers using regex\n",
    "#         match = re.search(r'\\d+', value)\n",
    "#         if match:\n",
    "#             return int(match.group())\n",
    "#     elif isinstance(value, (int, float)):\n",
    "#         # Return directly if it's already a number\n",
    "#         return int(value)\n",
    "    \n",
    "#     # Return None or a default value if no number is found\n",
    "#     return None\n",
    "\n",
    "# df['exa_quality_score'] = df['exa_quality_score'].apply(clean_column_value)\n",
    "# df['perplexity_quality_score'] = df['perplexity_quality_score'].apply(clean_column_value)\n",
    "# df['gemini_quality_score'] = df['gemini_quality_score'].apply(clean_column_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "quotient",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
